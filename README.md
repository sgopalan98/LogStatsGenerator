## Santhanagopalan Krishnamoorthy

---

## CS 441 - Engineering Distributed Objects for Cloud Computing

---

## Homework 2 - Map Reduce jobs to calculate statistics for Log file

---

### Overview

1. The objective of this homework was to process the log file generated by [LogFileGenerator](https://github.com/0x1DOCD00D/LogFileGenerator) using **Hadoop Map-Reduce framework** to produce various statistics, which are run by various jobs.
2. Link for deployment of JAR, Input file and running of Jobs on AWS EMR: 

### Instructions

#### Environment

The project was developed using the following environment:

- **OS:** MacOS BigSur 14.6
- **IDE:** IntelliJ IDEA Ultimate 2021.2.1
- **Hypervisor:** VirtualBox 
- **Hadoop Distribution:** [Hortonworks Data Platform (3.0.1) Sandbox](https://hortonworks.com/products/sandbox/) deployed on VMware

#### Prerequisites

- [HDP Sandbox](https://hortonworks.com/products/sandbox/) set up and deployed on (VMware or VirtualBox). Read this [guide](https://hortonworks.com/tutorial/learning-the-ropes-of-the-hortonworks-sandbox/) for instructions on how to set up and use HDP Sandbox
- Ability to use SSH and SCP on your system
- [SBT](https://www.scala-sbt.org/) installed on your system
- [LogFileGenerator](https://github.com/0x1DOCD00D/LogFileGenerator) cloned and run using sbt to generate the log file.

#### Running the map reduce job

1. Clone [LogFileGenerator](https://github.com/0x1DOCD00D/LogFileGenerator) and run the sbt tasks.
```
sbt test run
```
2. Clone this repository
3. Define the predefined intervals in the configuration in the repository with values from the log file generated. (Or use the sample log provided in resources/LogFileGenerator.2021-10-15.log, for which the intervals are predefined in resources/application.conf)
4. Generate the fat jar file using SBT on shell / cmd

```
sbt clean compile assembly
```

4. Start HDP sandbox VM
5. Copy the jar file to HDP Sandbox VM

```
scp -P 2222 ./target/scala-3.0.2/LogFileStats-assembly-0.1.jar root@sandbox-hdp.hortonworks.com:~/
```

6. Copy the LogFile to HDP Sandbox VM(Replace %LOGFILE_GENERATOR% to path of logfilegenerator and %LOGFILENAME% with logfilename)

```
scp -P 2222 % %LOGFILE_GENERATOR%/log/%LOGFILENAME% root@sandbox-hdp.hortonworks.com:~/
```

7. SSH into HDP Sandbox

```
ssh -p 2222 root@sandbox-hdp.hortonworks.com
```

8. Create input directory on HDFS and copy %LOGFILENAME% there

```

hdfs dfs -mkdir -p /user/root/Input/
hdfs dfs -put %LOGFILENAME% /user/root/Input/

```


9. Start the map-reduce job by mentioning the input, output and the job number in this format.

```
hadoop jar LogFileStats-assembly-0.1.jar /user/root/Input/%LOGFILENAME% /user/root/Output/ 1
```

### Output and explanation of MapReduce jobs

### Task 1: Produce a CSV of distribution of LOG type messages in all predefined intervals.

```
hadoop jar LogFileStats-assembly-0.1.jar /user/root/Input/%LOGFILENAME% /user/root/Output/ 1
```

```

```


#### Explanation:

- The predefined intervals have been defined in the configuration files. We need to calculate the number of each LOG TYPE messages in each of these intervals. This can be achieved in parallel by MapReduce.
- In the Mappers, we take line by line and Split it to find the Log type, which interval it falls in and the log message. We emit (INTERVAL+" "+LogType, 1) as the Key-Value pair from the Mapper job.
- In the Reducers, we get each Interval-LogType as the key and a list of 1s, which we aggregate the find the number of LogType message in that interval.


### Task 2: Produce a CSV that contains the sorted interval range in terms of no of ERROR messages with the injected Regex pattern.

```
hadoop jar LogFileStats-assembly-0.1.jar /user/root/Input/%LOGFILENAME% /user/root/Output/ 2
```

```

```

#### Explanation:

- The predefined intervals have been defined in the configuration files. We need to calculate how many ERROR messages with the regex string have been calculated and sort it. This can be done in two Map-Reduce jobs.
- In the FirstJob-Mappers, we take every line and check if it is of the ERROR message. If it does, we check if the Regex pattern is present. If it is present, we compute the interval and emit(INTERVAL, 1).
- In the FirstJob-Reducers, we get each Interval as the key and a list of 1s, which we aggregate the find the number of LogType message in that interval. 
- At the end of FirstJob, we have computed (INTERVAL, NO OF LOG MESSAGES CONTAINING ERROR).
- In the SecondJob, we set the no of reducer count to 1, for all the keys to fall in the same node with the descending order sorted.
- In the SecondJob-Mappers, we swap the Key and Value, for the reducer to have them sorted. (Reducer performs a sort on the Keys before it starts executing)
- In the SecondJob-Reducer, we swap the Key and Value back again, to have it in the format(INTERVAL, NO OF ERROR MESSAGE) and write it to a CSV.

### Task 3: Produce a CSV of distribution of LOG type messages.

```
hadoop jar LogFileStats-assembly-0.1.jar /user/root/Input/%LOGFILENAME% /user/root/Output/ 3
```
```

```

#### Explanation:

- In this Job, We need to calculate the number of LOG TYPE messages. This can be achieved in parallel by a simple MapReduce.
- In the Mapper, we take line by line and Split it to find the Log type. We emit (LogType, 1) as the Key-Value pair from the Mapper job.
- In the Reducer, we get each LogType as the key and a list of 1s, which we aggregate the find the number of LogType message in that interval.


### Task 4: Produce a CSV of distribution of LOG type messages in all predefined intervals.

```
hadoop jar LogFileStats-assembly-0.1.jar /user/root/Input/%LOGFILENAME% /user/root/Output/ 4
```
```

```

#### Explanation:

- The predefined intervals have been defined in the configuration files. We need to calculate the longest log message length with the Regex pattern. This can be achieved in parallel by MapReduce.
- In the Mapper, we take line by line and Split it to find the Log type and its length. We emit (LogType, length) as the Key-Value pair from the Mapper job.
- In the Reducer, we get each LogType as the key and a list of lengths, from which we find the maximum and write it as (LogType, length).


#### Improvements for the future and Credits

1.  